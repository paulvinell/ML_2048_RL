{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double deep Q-learning in 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game imports\n",
    "This is made slightly more complicated than it has to be, because the target folder has characters that are disallowed in python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "dir_path = os.path.abspath('')\n",
    "dir_path = os.path.join(dir_path, '2048-python-custom-player')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, dir_path)\n",
    "\n",
    "import constants as c\n",
    "from tie_in import TieIn # Used to launch a game of 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set game constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.GRID_LEN_X = 4\n",
    "c.GRID_LEN_Y = 4\n",
    "c.PROBABILITY_4 = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, InputLayer, Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear previous models from memory (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME_SHAPE = (c.GRID_LEN_X, c.GRID_LEN_Y, 1)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential([InputLayer(input_shape=GAME_SHAPE),\n",
    "                         Conv2D(32, kernel_size=2, activation='relu'),\n",
    "                        Conv2D(32, kernel_size=2, activation='relu'),\n",
    "                        Conv2D(64, kernel_size=2, activation='relu'),\n",
    "                        Dense(64, activation='relu'),\n",
    "                        Dense(4, activation='softmax'),\n",
    "                        Reshape((4,))])\n",
    "\n",
    "    model.compile(loss=keras.losses.CategoricalCrossentropy(from_logits=False), optimizer=keras.optimizers.Adam())\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "model_copy = create_model()\n",
    "model_copy.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Because the growth rate of the tiles in 2048 is exponential, we take log2 of all values, since linearly scaling values are more manageable. We also squish the values to be smaller in scale. For instance, 2 maps to 0.1, 1024 to 1, 8192 to 1.3, and 0 to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reward_func(score_change):\n",
    "    if score_change == 0:\n",
    "        return 0\n",
    "    return np.log2(score_change) / np.log2(1024)\n",
    "\n",
    "def preprocess(matrix):\n",
    "    non_zeros = np.nonzero(matrix)\n",
    "    matrix[non_zeros] = np.log2(matrix[non_zeros]) / np.log2(1024)\n",
    "    return matrix\n",
    "\n",
    "def one_hot(moves):\n",
    "    move_count = moves.shape[0]\n",
    "    oh = np.zeros((move_count, 4))\n",
    "    oh[np.arange(move_count), moves] = 1\n",
    "    return oh\n",
    "\n",
    "ACTION_SPACE = np.arange(4)\n",
    "def sparse(moves):\n",
    "    return np.dot(moves, ACTION_SPACE).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "Because of rotational symmetry, each game of 2048 corresponds to another set of three games (at least for square games). If we accept a move in one of the games as valid, then state and action's corresponding rotations must be equally valid. We use this argument to create more artificial games. Hopefully, this will also prevent a collapse in strategy where the model favors one corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_rotation_lookup = { 0: 2, 1: 3, 2: 1, 3: 0} # UP->LEFT, DOWN->RIGHT, LEFT->DOWN, RIGHT->UP\n",
    "def rotate_90_moves(moves):\n",
    "    move_count = moves.shape[0]\n",
    "    rot = np.zeros(moves.shape)\n",
    "    for i in range(move_count): # TODO: this is likely an inefficient way to implement this\n",
    "        rot[i] = move_rotation_lookup[rot[i]]\n",
    "    return rot.astype(int)\n",
    "\n",
    "def rotate_90_board(boards):\n",
    "    return np.rot90(boards, axes=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_model_input(matrix):\n",
    "    model_input = np.array(matrix)\n",
    "    model_input = np.expand_dims(model_input, axis=-1) # Number of channels (1)\n",
    "    model_input = np.expand_dims(model_input, axis=0) # Batch dimension\n",
    "    return model_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    def __init__(self, max_length):\n",
    "        self.max_length = max_length\n",
    "        self.state_memory_before = np.zeros((self.max_length, *GAME_SHAPE))\n",
    "        self.state_memory_after = np.zeros((self.max_length, *GAME_SHAPE))\n",
    "        self.action_memory = np.zeros((self.max_length, 4))\n",
    "        self.reward_memory = np.zeros((self.max_length))\n",
    "        self.terminal_memory = np.zeros((self.max_length))\n",
    "        self.counter = 0\n",
    "        \n",
    "    def store(self, state_before, state_after, action, score_diff, tile_count_diff, done):\n",
    "        state_before = matrix_to_model_input(state_before)\n",
    "        state_after = matrix_to_model_input(state_after)\n",
    "        action = np.array([action])\n",
    "        reward = reward_func(score_diff)\n",
    "\n",
    "        for rot in range(4):\n",
    "            index = self.counter % self.max_length\n",
    "\n",
    "            self.state_memory_before[index] = state_before\n",
    "            self.state_memory_after[index] = state_after\n",
    "            self.action_memory[index, :] = one_hot(action)[0, :]\n",
    "            self.reward_memory[index] = reward\n",
    "            self.terminal_memory[index] = 1 - done\n",
    "\n",
    "            self.counter += 1            \n",
    "            \n",
    "            if rot < 3:\n",
    "                state_before = rotate_90_board(state_before)\n",
    "                state_after = rotate_90_board(state_after)\n",
    "                action = rotate_90_moves(action)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        max_length = np.minimum(self.max_length, self.counter)\n",
    "        batch = np.random.choice(max_length, batch_size)\n",
    "        states_before = self.state_memory_before[batch]\n",
    "        states_after = self.state_memory_after[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "        \n",
    "        return states_before, states_after, actions, rewards, terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a custom player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9995\n",
    "EPSILON = 1.0\n",
    "EPSILON_DEC = 0.9995\n",
    "EPSILON_END = 0.01\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPlayer():\n",
    "    def __init__(self, model, model_copy):\n",
    "        # Hyperparameters\n",
    "        self.gamma = GAMMA # How much to discount future rewards\n",
    "        self.epsilon = EPSILON # Controls the probability of exploit/explore\n",
    "        self.epsilon_dec = EPSILON_DEC # How fast epsilon decreases\n",
    "        self.epsilon_end = EPSILON_END # Minimum epsilon value\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        \n",
    "        # Model\n",
    "        self.model = model\n",
    "        self.model_copy = model_copy\n",
    "        \n",
    "        # Experience\n",
    "        self.memory = ExperienceReplay(1000000)\n",
    "        \n",
    "        # Other\n",
    "        self.game_final = None # Contains the last state of the game\n",
    "        self.loss = -1\n",
    "        self.reset_game_specific_variables()\n",
    "        self.training = True\n",
    "    \n",
    "    def reset_game_specific_variables(self):\n",
    "        self.last_state = None\n",
    "        self.last_score = -1\n",
    "        self.last_action = -1\n",
    "        self.last_turn = -1\n",
    "    \n",
    "    def game_grid_init(self, game_grid):\n",
    "        self.reset_game_specific_variables()\n",
    "        self.training = False\n",
    "\n",
    "    def play(self, game):\n",
    "        ### Check that we picked a valid move. ###\n",
    "        if game.move_count > self.last_turn:\n",
    "            self.last_turn = game.move_count\n",
    "        else:\n",
    "            print(\"ModelPlayer: stuck on turn {}\".format(game.move_count))\n",
    "            breakpoint()\n",
    "            \n",
    "        ### Store experience. ###\n",
    "        if game.move_count > 0:\n",
    "            state_before = self.last_state\n",
    "            state_after = game.matrix\n",
    "            action = self.last_action\n",
    "            score_diff = game.score_diff\n",
    "            tile_count_diff = game.tile_count_diff\n",
    "            done = 0\n",
    "            self.memory.store(state_before, state_after, action, score_diff, tile_count_diff, done)\n",
    "        \n",
    "        ### Pick a move ###\n",
    "        possible_directions = game.possible_directions()\n",
    "        possible_directions = np.array(possible_directions)\n",
    "        \n",
    "        explore_flag = (np.random.random() < self.epsilon) and self.training\n",
    "        \n",
    "        ### Let the model suggest a move ###\n",
    "        if not explore_flag:\n",
    "            model_input = matrix_to_model_input(game.matrix)\n",
    "            model_input = preprocess(model_input)\n",
    "            \n",
    "            model_output = self.model.predict(model_input)\n",
    "\n",
    "            model_mask = np.zeros(4)\n",
    "            model_mask[possible_directions] = 1\n",
    "            model_output *= model_mask # Only select from possible moves\n",
    "\n",
    "            # Pick a move based on how strongly the model suggests it\n",
    "            response = np.argmax(model_output)\n",
    "        \n",
    "        ### Pick a random move in case of explore or calculation failure ###\n",
    "        if (explore_flag\n",
    "            or response is None\n",
    "            or not np.any(possible_directions == response)):\n",
    "            response = np.random.choice(possible_directions)\n",
    "        \n",
    "        ### Short-term state storage to be able to calculate rewards ###\n",
    "        self.last_state = game.matrix\n",
    "        self.last_score = game.score\n",
    "        self.last_action = response\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def sleep(self, game, render):\n",
    "        if not render:\n",
    "            return 0 # Don't sleep when training\n",
    "\n",
    "        return np.log2(game.max_tile) / 30 # Go slower when it gets interesting\n",
    "    \n",
    "    def lost(self, game):\n",
    "        self.game_final = game\n",
    "        \n",
    "        ### Store experience. ###\n",
    "        if self.training:\n",
    "            state_before = self.last_state\n",
    "            state_after = game.matrix\n",
    "            action = self.last_action\n",
    "            score_diff = game.score_diff\n",
    "            tile_count_diff = game.tile_count_diff\n",
    "            done = 1\n",
    "            self.memory.store(state_before, state_after, action, score_diff, tile_count_diff, done)\n",
    "\n",
    "            self.reset_game_specific_variables() # Clean up to prepare for next game\n",
    "            self.learn()\n",
    "        else:\n",
    "            self.training = True\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.memory.counter < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states_before, states_after, actions, reward, done = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        action_indices = sparse(actions)\n",
    "        \n",
    "        states_before = preprocess(states_before)\n",
    "        states_before = preprocess(states_after)\n",
    "        \n",
    "        pred_first = self.model.predict(states_before)\n",
    "        pred_second = self.model_copy.predict(states_after)\n",
    "        \n",
    "        q_target = pred_first.copy()\n",
    "        q_target = np.squeeze(q_target)\n",
    "        batch_index = np.arange(self.batch_size, dtype=int)\n",
    "        q_target[batch_index, action_indices] = reward + self.gamma * np.max(pred_second, axis=1)*done\n",
    "\n",
    "        history = self.model.fit(states_before, q_target, verbose=0)\n",
    "        self.loss = history.history['loss'][-1]\n",
    "        \n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon *= self.epsilon_dec\n",
    "            self.epsilon = np.maximum(self.epsilon, self.epsilon_end)\n",
    "    \n",
    "    def update_model_copy(self):\n",
    "        self.model_copy.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GAMES = 2000\n",
    "MODEL_COPY_UPDATE_FREQUENCY = 5\n",
    "MOVING_AVERAGE_N = 10 # Moving average setting\n",
    "PRINT_INTERVALS = 2 # Number of seconds between progress update (minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "from functools import reduce\n",
    "\n",
    "last_print_time = time.time()\n",
    "history = []\n",
    "avg_num_moves = -1\n",
    "avg_loss = -1\n",
    "moving_average_factor = 1.0 / MOVING_AVERAGE_N\n",
    "\n",
    "model_player = ModelPlayer(model, model_copy)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for game_number in range(1, NUM_GAMES + 1):\n",
    "    ti = TieIn(model_player, render=False, log_history=False)\n",
    "    ti.start()\n",
    "    \n",
    "    if game_number == 1:\n",
    "        avg_num_moves = model_player.game_final.move_count\n",
    "        avg_loss = model_player.loss\n",
    "    else:\n",
    "        avg_num_moves = avg_num_moves*(1-moving_average_factor) + model_player.game_final.move_count*moving_average_factor\n",
    "        avg_loss = avg_loss*(1-moving_average_factor) + model_player.loss*moving_average_factor\n",
    "        \n",
    "        if (game_number - 1) % MODEL_COPY_UPDATE_FREQUENCY == 0:\n",
    "            model_player.update_model_copy()\n",
    "        \n",
    "    cur_time = time.time()\n",
    "    if cur_time - last_print_time >= PRINT_INTERVALS:\n",
    "        print(\"Status: Games {}/{}: {}-game moving average: {:.1f} steps, {:.5g} loss, {:.5g} epsilon\".format(game_number, NUM_GAMES, MOVING_AVERAGE_N, avg_num_moves, avg_loss, model_player.epsilon))\n",
    "        last_print_time = cur_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model and training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model.save('q_learning.h5')\n",
    "\n",
    "policy_gradient_history = { 'history': history }\n",
    "file = open('q_learning_history.pickle', 'wb')\n",
    "pickle.dump(q_learning_history, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('q_learning.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti = TieIn(model_player, render=True, log_history=False)\n",
    "ti.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
